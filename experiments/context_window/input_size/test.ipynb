{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.getcwd() + '/../..')\n",
    "print(sys.path)\n",
    "print(module_path)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "print(os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "sys.path.append(module_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from deepeval.dataset import Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "\n",
    "from benchmarq.utility import VLLMServerSingleton\n",
    "from benchmarq.utility import Evaluator\n",
    "from benchmarq.experiment import Experiment\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "csv_files = [\"processed_OpenCaselist-100.csv\",\n",
    "             \"processed_OpenCaselist-500.csv\",\n",
    "             \"processed_OpenCaselist-1000.csv\",\n",
    "             \"processed_OpenCaselist-2000.csv\",\n",
    "             \"processed_OpenCaselist-3000.csv\",\n",
    "             \"processed_OpenCaselist-4000.csv\",\n",
    "             \"processed_OpenCaselist-5000.csv\",\n",
    "             \"processed_OpenCaselist-10000.csv\",\n",
    "             \"processed_OpenCaselist-20000.csv\", ]\n",
    "\n",
    "server = VLLMServerSingleton()\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "async_client = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "server.start_server(model=\"EleutherAI/pythia-1.4b\")\n",
    "\n",
    "class Test(Evaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    async def async_evaluate_consumption(self, input: Golden):\n",
    "        return await async_client.completions.create(model=\"EleutherAI/pythia-1.4b\", prompt=f\"{input.context}: {input.input}\")\n",
    "\n",
    "    def evaluate_test_case(self, input: Golden) -> LLMTestCase:\n",
    "        output = client.completions.create(model=\"EleutherAI/pythia-1.4b\", prompt=f\"{input.context}: {input.input}\").choices[0].text\n",
    "        return LLMTestCase(input=input.input, expected_output=input.expected_output, actual_output=output, context=input.context, retrieval_context=input.retrieval_context)\n",
    "\n",
    "experiment = Experiment(\n",
    "    subquestion_id=\"test_3\",\n",
    "    id=\"case_1\",\n",
    "    dataset_name=\"beatles\",\n",
    "    subquestion_path=\"experiments/test/tests.json\",\n",
    "    name=\"name\",\n",
    "    description=\"A very long description\",\n",
    "    settings=Test())\n",
    "\n",
    "a = await experiment.run()\n",
    "\n",
    "server.stop_server()"
   ],
   "id": "d669fbfa1a0781be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
