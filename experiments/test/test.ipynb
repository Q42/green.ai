{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-04T08:59:07.339493Z",
     "start_time": "2025-03-04T08:58:05.944250Z"
    }
   },
   "source": [
    "from pydantic import Field\n",
    "from deepeval.dataset import Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from benchmarq.utility import Evaluator\n",
    "from benchmarq.experiment import Experiment\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "def generate(prompt: str) -> str:\n",
    "    return generator(prompt)\n",
    "\n",
    "\n",
    "class Test(Evaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def evaluate_consumption(self, input: Golden):\n",
    "        generate(input.input)\n",
    "\n",
    "    def evaluate_test_case(self, input: Golden) -> LLMTestCase:\n",
    "        output = generate(input.input)[0][\"generated_text\"]\n",
    "        print(output)\n",
    "        return LLMTestCase(input=input.input, expected_output=input.expected_output, actual_output=output, context=input.context, retrieval_context=input.retrieval_context)\n",
    "\n",
    "experiment = Experiment(\n",
    "    subquestion_id=\"subquestionId\",\n",
    "    subquestion_path=\"experiments/test/tests.json\",\n",
    "    name=\"name\",\n",
    "    description=\"A very long description\",\n",
    "    settings=Test())\n",
    "\n",
    "print(experiment.run())\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/green.ai/lib/python3.11/site-packages/deepeval/__init__.py:53: UserWarning: You are using deepeval version 2.4.8, however version 2.4.9 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "Device set to use mps:0\n",
      "[codecarbon INFO @ 09:58:22] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 09:58:22] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 09:58:22] No GPU found.\n",
      "[codecarbon INFO @ 09:58:22] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 09:58:22] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 09:58:23] CPU Model on constant consumption mode: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 09:58:23] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 09:58:23]   Platform system: macOS-15.3.1-x86_64-i386-64bit\n",
      "[codecarbon INFO @ 09:58:23]   Python version: 3.11.0\n",
      "[codecarbon INFO @ 09:58:23]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 09:58:23]   Available RAM : 16.000 GB\n",
      "[codecarbon INFO @ 09:58:23]   CPU count: 16\n",
      "[codecarbon INFO @ 09:58:23]   CPU model: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 09:58:23]   GPU count: None\n",
      "[codecarbon INFO @ 09:58:23]   GPU model: None\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "[codecarbon INFO @ 09:58:51] Energy consumed for RAM : 0.000025 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:58:51] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:58:51] 0.000119 kWh of electricity used since the beginning.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "[codecarbon INFO @ 09:58:55] Energy consumed for RAM : 0.000031 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 09:58:55] Energy consumed for all CPUs : 0.000116 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 09:58:55] 0.000147 kWh of electricity used since the beginning.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something for my little friend. This is the easiest, easiest way to try out for me. No effort needed. Get this on my phone right now.\n",
      "\n",
      "Rated 4 out of 5 by Anonymous from Pretty easy to use. I've been using\n",
      "Lucy said. \"I really do feel like I understand what she is going through. I'm not in that situation anymore. It's nothing more. I am not dealing with life.\"\n",
      "\n",
      "Sutton has said he will appeal when he takes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001B[38;2;106;0;255mCorrectness \u001B[0m\u001B[1;38;2;106;0;255m(\u001B[0m\u001B[38;2;106;0;255mGEval\u001B[0m\u001B[1;38;2;106;0;255m)\u001B[0m\u001B[38;2;106;0;255m Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing gpt-4o, \u001B[0m\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001B[38;2;106;0;255mSuccinctness \u001B[0m\u001B[1;38;2;106;0;255m(\u001B[0m\u001B[38;2;106;0;255mGEval\u001B[0m\u001B[1;38;2;106;0;255m)\u001B[0m\u001B[38;2;106;0;255m Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing gpt-4o, \u001B[0m\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Succinctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating 2 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (2/2) [Time Taken: 00:05,  2.58s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output shares no common elements or factual alignment with the expected output., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.0633848241008612, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The actual output contains multiple sentences, while the context and evaluation steps require exactly one sentence., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Something\n",
      "  - actual output: Something for my little friend. This is the easiest, easiest way to try out for me. No effort needed. Get this on my phone right now.\n",
      "\n",
      "Rated 4 out of 5 by Anonymous from Pretty easy to use. I've been using\n",
      "  - expected output: in the way she moves.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output does not share any factual elements with the expected output, containing irrelevant narrative details. The expected output is a fragmentary phrase, while the actual output introduces multiple characters and situations not present in the expected content., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.012046984199057996, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The actual output contains multiple sentences, which violates the requirement of a single-sentence output, and the context does not justify any deviation from this rule., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Lucy\n",
      "  - actual output: Lucy said. \"I really do feel like I understand what she is going through. I'm not in that situation anymore. It's nothing more. I am not dealing with life.\"\n",
      "\n",
      "Sutton has said he will appeal when he takes\n",
      "  - expected output: in the sky with diamonds.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness (GEval): 0.00% pass rate\n",
      "Succinctness (GEval): 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m‚úì\u001B[0m Tests finished üéâ! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunResult(consumption_results=ConsumptionResult(timestamp='2025-03-04T09:58:55', project_name='codecarbon', run_id='36aef877-b70e-483e-8477-30877f79605c', experiment_id='1', duration=18.61198592185974, emissions=4.8784921971700495e-05, emissions_rate=2.6211561827157145e-06, cpu_power=22.5, gpu_power=0.0, ram_power=6.0, cpu_energy=0.00011630655080080034, gpu_energy=0.0, ram_energy=3.1011824607849124e-05, energy_consumed=0.00014731837540864944, country_name='The Netherlands', country_iso_code='NLD', region='north brabant', cloud_provider='', cloud_region='', os='macOS-15.3.1-x86_64-i386-64bit', python_version='3.11.0', codecarbon_version='2.2.2', cpu_count=16.0, cpu_model='Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz', gpu_count=None, gpu_model=None, longitude=5.0661, latitude=51.5542, ram_total_size=16.0, tracking_mode='machine', on_cloud='N', pue=1), metric_results=[TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=False, score=0.0, reason='The actual output shares no common elements or factual alignment with the expected output.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0022425, verbose_logs='Criteria:\\nDetermine whether the actual output is factually correct based on the expected output \\n \\nEvaluation Steps:\\n[\\n    \"Compare the actual output to the expected output to identify any discrepancies.\",\\n    \"Check if the actual output factually aligns with the expected output.\",\\n    \"Assess the underlying data or logic used in the actual output to determine factual correctness.\",\\n    \"Verify that all critical elements in the expected output are accurately represented in the actual output.\"\\n]'), MetricData(name='Succinctness (GEval)', threshold=0.7, success=False, score=0.0633848241008612, reason='The actual output contains multiple sentences, while the context and evaluation steps require exactly one sentence.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00232, verbose_logs='Criteria:\\nDetermine whether the actual output is exactly one sentence and not more \\n \\nEvaluation Steps:\\n[\\n    \"Verify that the input is provided and valid for the context.\",\\n    \"Check if the actual output is a single sentence by identifying the presence and correct use of punctuation marks.\",\\n    \"Confirm that the context requires the response to be exactly one sentence.\",\\n    \"Evaluate if the actual output satisfies the requirement of being exactly one sentence in relation to the given context.\"\\n]')], conversational=False, multimodal=False, input='Something', actual_output=\"Something for my little friend. This is the easiest, easiest way to try out for me. No effort needed. Get this on my phone right now.\\n\\nRated 4 out of 5 by Anonymous from Pretty easy to use. I've been using\", expected_output='in the way she moves.', context=[], retrieval_context=[]), TestResult(name='test_case_1', success=False, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=False, score=0.0, reason='The actual output does not share any factual elements with the expected output, containing irrelevant narrative details. The expected output is a fragmentary phrase, while the actual output introduces multiple characters and situations not present in the expected content.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0025975, verbose_logs='Criteria:\\nDetermine whether the actual output is factually correct based on the expected output \\n \\nEvaluation Steps:\\n[\\n    \"Verify that the actual output contains the same key factual elements as the expected output.\",\\n    \"Compare the details in the actual output to the expected output to ensure factual consistency.\",\\n    \"Identify any discrepancies between the actual output and expected output regarding facts and data.\",\\n    \"Confirm that the actual output does not introduce any factual errors not found in the expected output.\"\\n]'), MetricData(name='Succinctness (GEval)', threshold=0.7, success=False, score=0.012046984199057996, reason='The actual output contains multiple sentences, which violates the requirement of a single-sentence output, and the context does not justify any deviation from this rule.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00243, verbose_logs='Criteria:\\nDetermine whether the actual output is exactly one sentence and not more \\n \\nEvaluation Steps:\\n[\\n    \"Check if the actual output contains only one sentence by counting sentence-ending punctuation.\",\\n    \"Verify whether the input necessitates a single-sentence output by consulting the context requirements.\",\\n    \"Compare the actual output against the context specifications to ensure alignment in sentence count.\",\\n    \"Confirm that any deviations from the one-sentence rule are justified by the context if applicable.\"\\n]')], conversational=False, multimodal=False, input='Lucy', actual_output='Lucy said. \"I really do feel like I understand what she is going through. I\\'m not in that situation anymore. It\\'s nothing more. I am not dealing with life.\"\\n\\nSutton has said he will appeal when he takes', expected_output='in the sky with diamonds.', context=[], retrieval_context=[])], timestamp=datetime.datetime(2025, 3, 4, 9, 59, 7, 336244))\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T14:35:06.340487Z",
     "start_time": "2025-03-03T14:35:06.333750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "a=pd.read_csv(\"/Users/leenlaptop/Documents/repos/greenai/green.ai/experiments/test/inputs.csv\")\n",
    "\n",
    "print(a)"
   ],
   "id": "ab77be765e1ef830",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       input            expected_output  \\\n",
      "0  Something      in the way she moves.   \n",
      "1       Lucy  in the sky with diamonds.   \n",
      "\n",
      "                                             context  retrieval_context  \\\n",
      "0  finish the sentence of the song of the beatles...                NaN   \n",
      "1  finish the sentence of the song of the beatles...                NaN   \n",
      "\n",
      "   tools_called  expected_tools  \n",
      "0           NaN             NaN  \n",
      "1           NaN             NaN  \n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
