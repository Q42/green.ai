{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb229d1-dbd0-4c4d-8bc4-fe6d10871a24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/envs/green.ai/lib/python311.zip', '/opt/conda/envs/green.ai/lib/python3.11', '/opt/conda/envs/green.ai/lib/python3.11/lib-dynload', '', '/opt/conda/envs/green.ai/lib/python3.11/site-packages', '/opt/conda/envs/green.ai/lib/python3.11/site-packages/setuptools/_vendor', '/var/tmp/tmpwovsk3yb', '/home/jupyter/green.ai/experiments', '/home/jupyter/green.ai/experiments', '/home/jupyter/green.ai/experiments', '/home/jupyter/green.ai/experiments']\n",
      "/home/jupyter/green.ai\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "module_path = os.path.abspath(os.getcwd()+'/../..')\n",
    "print(sys.path)\n",
    "print(module_path)\n",
    "\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T09:35:39.615770Z",
     "start_time": "2025-03-05T09:34:49.768384Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/green.ai/lib/python3.11/site-packages/deepeval/__init__.py:54: UserWarning: You are using deepeval version 2.5.7, however version 2.5.8 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'benchmarq'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepeval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtest_case\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMTestCase\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbenchmarq\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Evaluator\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbenchmarq\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Experiment\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'benchmarq'"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "from deepeval.dataset import Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from benchmarq.utility import Evaluator\n",
    "from benchmarq.experiment import Experiment\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "def generate(prompt: str) -> str:\n",
    "    return generator(prompt)\n",
    "\n",
    "\n",
    "class Test(Evaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def evaluate_consumption(self, input: Golden):\n",
    "        generate(input.input)\n",
    "\n",
    "    def evaluate_test_case(self, input: Golden) -> LLMTestCase:\n",
    "        output = generate(f\"{input.context}: {input.input}\")[0][\"generated_text\"]\n",
    "        return LLMTestCase(input=input.input, expected_output=input.expected_output, actual_output=output, context=input.context, retrieval_context=input.retrieval_context)\n",
    "\n",
    "experiment = Experiment(\n",
    "    subquestion_id=\"test_1\",\n",
    "    id=\"case_1\",\n",
    "    subquestion_path=\"experiments/test/tests.json\",\n",
    "    name=\"name\",\n",
    "    description=\"A very long description\",\n",
    "    settings=Test())\n",
    "\n",
    "a=experiment.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77be765e1ef830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "a=pd.read_csv(\"/Users/leenlaptop/Documents/repos/greenai/green.ai/experiments/test/inputs.csv\")\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e765952bba984711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T14:59:26.713570Z",
     "start_time": "2025-03-04T14:59:26.704808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subquestion_id': 'test',\n",
       " 'subquestion_metrics_path': 'experiments/test/tests.json',\n",
       " 'experiments': [{'id': '8c30631fad934d5da404ad35ac98468e',\n",
       "   'name': 'name',\n",
       "   'description': 'A very long description',\n",
       "   'settings': {},\n",
       "   'runs': [{'consumption_results': {'cloud_provider': '',\n",
       "      'cloud_region': '',\n",
       "      'codecarbon_version': '2.2.2',\n",
       "      'country_iso_code': 'NLD',\n",
       "      'country_name': 'The Netherlands',\n",
       "      'cpu_count': 16,\n",
       "      'cpu_energy': 0.00010849921256303787,\n",
       "      'cpu_model': 'Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz',\n",
       "      'cpu_power': 22.5,\n",
       "      'duration': 17.361709117889404,\n",
       "      'emissions': 4.551006183701586e-05,\n",
       "      'emissions_rate': 2.621289271004002e-06,\n",
       "      'energy_consumed': 0.00013742910931507747,\n",
       "      'experiment_id': '1',\n",
       "      'gpu_count': None,\n",
       "      'gpu_energy': 0,\n",
       "      'gpu_model': None,\n",
       "      'gpu_power': 0.0,\n",
       "      'latitude': 51.5542,\n",
       "      'longitude': 5.0661,\n",
       "      'on_cloud': 'N',\n",
       "      'os': 'macOS-15.3.1-x86_64-i386-64bit',\n",
       "      'project_name': 'codecarbon',\n",
       "      'pue': 1,\n",
       "      'python_version': '3.11.0',\n",
       "      'ram_energy': 2.8929896752039596e-05,\n",
       "      'ram_power': 6.0,\n",
       "      'ram_total_size': 16.0,\n",
       "      'region': 'north brabant',\n",
       "      'run_id': '96d65168-5fe4-4fd7-acf6-ebfade7a7fd6',\n",
       "      'timestamp': '2025-03-04T15:58:57',\n",
       "      'tracking_mode': 'machine'},\n",
       "     'metric_results': [{'actual_output': \"[]: Something went wrong. There was something wrong with our system. It was not helping. (pause)\\n\\n[19:03] Guest: How could we possibly tell that from scratch if it's a separate operating system?\\n\\n[\",\n",
       "       'context': [],\n",
       "       'conversational': False,\n",
       "       'expected_output': 'in the way she moves.',\n",
       "       'input': 'Something',\n",
       "       'metrics_data': [{'error': None,\n",
       "         'evaluation_cost': 0.0021574999999999997,\n",
       "         'evaluation_model': 'gpt-4o',\n",
       "         'name': 'Correctness (GEval)',\n",
       "         'reason': 'The actual output and expected output have completely unrelated content with no factual alignment, failing all evaluation steps.',\n",
       "         'score': 0.0017986207395942124,\n",
       "         'strict_mode': False,\n",
       "         'success': False,\n",
       "         'threshold': 0.5,\n",
       "         'verbose_logs': 'Criteria:\\nDetermine whether the actual output is factually correct based on the expected output \\n \\nEvaluation Steps:\\n[\\n    \"Compare the actual output to the expected output to identify any factual discrepancies.\",\\n    \"Verify that all facts stated in the actual output match those in the expected output.\",\\n    \"Assess whether any important details present in the expected output are missing or inaccurately represented in the actual output.\"\\n]'},\n",
       "        {'error': None,\n",
       "         'evaluation_cost': 0.0023875,\n",
       "         'evaluation_model': 'gpt-4o',\n",
       "         'name': 'Succinctness (GEval)',\n",
       "         'reason': 'The actual output contains multiple sentences instead of exactly one, failing to meet the criteria.',\n",
       "         'score': 0.002197388505878706,\n",
       "         'strict_mode': False,\n",
       "         'success': False,\n",
       "         'threshold': 0.7,\n",
       "         'verbose_logs': 'Criteria:\\nDetermine whether the actual output is exactly one sentence and not more \\n \\nEvaluation Steps:\\n[\\n    \"Step 1: Identify the actual output from the text provided.\",\\n    \"Step 2: Analyze the actual output to determine the number of sentences it contains.\",\\n    \"Step 3: Compare the number of sentences in the actual output against the requirement of exactly one sentence.\",\\n    \"Step 4: Conclude if the actual output meets the criteria by confirming it contains precisely one sentence.\"\\n]'}],\n",
       "       'multimodal': False,\n",
       "       'name': 'test_case_0',\n",
       "       'retrieval_context': [],\n",
       "       'success': False},\n",
       "      {'actual_output': \"[]: Lucy is the girl who left when the girls woke up to find the city blocked by zombies. She is also the key to the world's map that unlocks the city on a first look.\",\n",
       "       'context': [],\n",
       "       'conversational': False,\n",
       "       'expected_output': 'in the sky with diamonds.',\n",
       "       'input': 'Lucy',\n",
       "       'metrics_data': [{'error': None,\n",
       "         'evaluation_cost': 0.002465,\n",
       "         'evaluation_model': 'gpt-4o',\n",
       "         'name': 'Correctness (GEval)',\n",
       "         'reason': 'The actual output has no factual relationship with the expected output; it introduces unrelated fictional elements about cities and zombies, while the expected output relates to a song lyric.',\n",
       "         'score': 0.0,\n",
       "         'strict_mode': False,\n",
       "         'success': False,\n",
       "         'threshold': 0.5,\n",
       "         'verbose_logs': 'Criteria:\\nDetermine whether the actual output is factually correct based on the expected output \\n \\nEvaluation Steps:\\n[\\n    \"Compare the actual output with the expected output to ensure factual consistency.\",\\n    \"Check if all facts present in the expected output are accurately represented in the actual output.\",\\n    \"Identify any discrepancies or deviations in facts between the actual output and expected output.\",\\n    \"Evaluate whether additional information in the actual output aligns factually with the context provided by the expected output.\"\\n]'},\n",
       "        {'error': None,\n",
       "         'evaluation_cost': 0.0020575000000000003,\n",
       "         'evaluation_model': 'gpt-4o',\n",
       "         'name': 'Succinctness (GEval)',\n",
       "         'reason': 'The actual output is not a single string; it contains two sentences. The input and context do not justify multiple sentences.',\n",
       "         'score': 0.06791182005781436,\n",
       "         'strict_mode': False,\n",
       "         'success': False,\n",
       "         'threshold': 0.7,\n",
       "         'verbose_logs': 'Criteria:\\nDetermine whether the actual output is exactly one sentence and not more \\n \\nEvaluation Steps:\\n[\\n    \"Check if the actual output is a single string.\",\\n    \"Split the actual output by sentence-ending punctuation.\",\\n    \"Ensure the split results in exactly one sentence.\",\\n    \"Confirm the input and additional context support a single-sentence output.\"\\n]'}],\n",
       "       'multimodal': False,\n",
       "       'name': 'test_case_1',\n",
       "       'retrieval_context': [],\n",
       "       'success': False}],\n",
       "     'timestamp': '2025-03-04T15:58:23.515991'}]}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.create_subquestion_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30bb598a68ff29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-18 08:29:05 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: vllm serve <model_tag> [options]\n",
      "vllm serve: error: the following arguments are required: model_tag\n"
     ]
    }
   ],
   "source": [
    "from benchmarq.utility import serve_vllm\n",
    " \n",
    "serve_vllm(model=\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee001e-c15f-41fc-a751-053b7d5322d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-green.ai-green.ai",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "KERNEL_DISPLAY_NAME (Local)",
   "language": "python",
   "name": "conda-env-green.ai-green.ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
