{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ceb7337d58aac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/envs/green.ai/lib/python311.zip', '/opt/conda/envs/green.ai/lib/python3.11', '/opt/conda/envs/green.ai/lib/python3.11/lib-dynload', '', '/opt/conda/envs/green.ai/lib/python3.11/site-packages']\n",
      "/home/jupyter/green.ai\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "module_path = os.path.abspath(os.getcwd()+'/../..')\n",
    "print(sys.path)\n",
    "print(module_path)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "print(os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc69cd2be14d2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server started successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:32:46] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:32:46] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:32:46] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 14:32:46] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:32:46] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 14:32:47] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 14:32:47] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "[codecarbon INFO @ 14:32:47] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:32:47]   Platform system: Linux-5.10.0-34-cloud-amd64-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 14:32:47]   Python version: 3.11.0\n",
      "[codecarbon INFO @ 14:32:47]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 14:32:47]   Available RAM : 14.649 GB\n",
      "[codecarbon INFO @ 14:32:47]   CPU count: 4\n",
      "[codecarbon INFO @ 14:32:47]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "[codecarbon INFO @ 14:32:47]   GPU count: 1\n",
      "[codecarbon INFO @ 14:32:47]   GPU model: 1 x Tesla T4\n",
      "[codecarbon INFO @ 14:32:48] Energy consumed for RAM : 0.000000 kWh. RAM Power : 5.493237018585205 W\n",
      "[codecarbon INFO @ 14:32:48] Energy consumed for all GPUs : 0.000005 kWh. Total GPU Power : 71.71600000000002 W\n",
      "[codecarbon INFO @ 14:32:48] Energy consumed for all CPUs : 0.000003 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:32:48] 0.000009 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Succinctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mSuccinctness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 4 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (4/4) [Time Taken: 00:04,  1.01s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: Actual Output and Expected Output share no common facts, leading to complete factual inaccuracy., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The actual output lacks a complete sentence and context is insufficient to imply a requirement for one., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Something\n",
      "  - actual output:  didn't work, re-enable xauth.\n",
      "<Unit193> sudo\n",
      "  - expected output: in the way she moves.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output is entirely different from the expected output, showing no factual alignment and missing key details., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.25127561631192114, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The output has two sentences due to two periods; it does not follow punctuation rules and lacks context to ensure task fulfillment., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Something\n",
      "  - actual output:  wrong with your browser. Please remove it and re-open the page you were\n",
      "  - expected output: in the way she moves.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output and the expected output do not match at all in content or context, failing to meet any of the evaluation criteria., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The Actual Output contains sentence fragments rather than a single complete sentence, and does not align with the input which is not sufficient context for a full sentence., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Lucy\n",
      "  - actual output:  hired keep you safe.\n",
      " * jacek__: okay, I'm\n",
      "  - expected output: in the sky with diamonds.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output does not relate to the expected output at all, failing to meet any of the evaluation criteria., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.30432814309946765, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The output contains text and consists of two sentences, but it shows no meaningful or relevant connection to the input 'Lucy' in the context provided., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Lucy\n",
      "  - actual output:  Knapp, RETL\n",
      "\n",
      "Looks like we just won the State Lottery\n",
      "  - expected output: in the sky with diamonds.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness (GEval): 0.00% pass rate\n",
      "Succinctness (GEval): 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server stopped.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "from deepeval.dataset import Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "\n",
    "from benchmarq.utility import VLLMServerSingleton\n",
    "from benchmarq.utility import Evaluator\n",
    "from benchmarq.experiment import Experiment\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "server = VLLMServerSingleton()\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "async_client = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "server.start_server(model=\"EleutherAI/pythia-1.4b\")\n",
    "\n",
    "class Test(Evaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    async def async_evaluate_consumption(self, input: Golden):\n",
    "        return await async_client.completions.create(model=\"EleutherAI/pythia-1.4b\", prompt=f\"{input.context}: {input.input}\")\n",
    "\n",
    "    def evaluate_test_case(self, input: Golden) -> LLMTestCase:\n",
    "        output = client.completions.create(model=\"EleutherAI/pythia-1.4b\", prompt=f\"{input.context}: {input.input}\").choices[0].text\n",
    "        return LLMTestCase(input=input.input, expected_output=input.expected_output, actual_output=output, context=input.context, retrieval_context=input.retrieval_context)\n",
    "\n",
    "experiment = Experiment(\n",
    "    subquestion_id=\"test_3\",\n",
    "    id=\"case_1\",\n",
    "    subquestion_path=\"experiments/test/tests.json\",\n",
    "    name=\"name\",\n",
    "    description=\"A very long description\",\n",
    "    settings=Test())\n",
    "\n",
    "a= await experiment.run()\n",
    "\n",
    "server.stop_server()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37be91acbe8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "a=pd.read_csv(\"/Users/leenlaptop/Documents/repos/greenai/green.ai/experiments/test/inputs.csv\")\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2482ac7fa67b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.create_subquestion_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79ef6d3de60514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "completion = client.completions.create(model=\"facebook/opt-125m\",\n",
    "                                      prompt=\"San Francisco is a\")\n",
    "\n",
    "print(\"Completion result:\", completion)\n",
    "\n",
    "time.sleep(10)  # Replace with actual usage logic\n",
    "server.stop_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961fad1ff3e30d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-green.ai-green.ai",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "KERNEL_DISPLAY_NAME (Local)",
   "language": "python",
   "name": "conda-env-green.ai-green.ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
