{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-04T09:16:51.473589Z",
     "start_time": "2025-03-04T09:15:54.560341Z"
    }
   },
   "source": [
    "from pydantic import Field\n",
    "from deepeval.dataset import Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from benchmarq.utility import Evaluator\n",
    "from benchmarq.experiment import Experiment\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "def generate(prompt: str) -> str:\n",
    "    return generator(prompt)\n",
    "\n",
    "\n",
    "class Test(Evaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def evaluate_consumption(self, input: Golden):\n",
    "        generate(input.input)\n",
    "\n",
    "    def evaluate_test_case(self, input: Golden) -> LLMTestCase:\n",
    "        output = generate(f\"{input.context}: {input.input}\")[0][\"generated_text\"]\n",
    "        print(output)\n",
    "        return LLMTestCase(input=input.input, expected_output=input.expected_output, actual_output=output, context=input.context, retrieval_context=input.retrieval_context)\n",
    "\n",
    "experiment = Experiment(\n",
    "    subquestion_id=\"subquestionId\",\n",
    "    subquestion_path=\"experiments/test/tests.json\",\n",
    "    name=\"name\",\n",
    "    description=\"A very long description\",\n",
    "    settings=Test())\n",
    "\n",
    "print(experiment.run())\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "[codecarbon INFO @ 10:15:57] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:15:57] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:15:57] No GPU found.\n",
      "[codecarbon INFO @ 10:15:57] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:15:57] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[codecarbon INFO @ 10:16:01] CPU Model on constant consumption mode: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 10:16:01] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:16:01]   Platform system: macOS-15.3.1-x86_64-i386-64bit\n",
      "[codecarbon INFO @ 10:16:01]   Python version: 3.11.0\n",
      "[codecarbon INFO @ 10:16:01]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 10:16:01]   Available RAM : 16.000 GB\n",
      "[codecarbon INFO @ 10:16:01]   CPU count: 16\n",
      "[codecarbon INFO @ 10:16:01]   CPU model: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 10:16:01]   GPU count: None\n",
      "[codecarbon INFO @ 10:16:01]   GPU model: None\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "[codecarbon INFO @ 10:16:26] Energy consumed for RAM : 0.000025 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:16:26] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 10:16:26] 0.000119 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:16:28] Energy consumed for RAM : 0.000028 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 10:16:28] Energy consumed for all CPUs : 0.000105 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 10:16:28] 0.000133 kWh of electricity used since the beginning.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]: Something went down that I don't know what but it's not my fault. I'm still at fault here and now I'm stuck, and the others are in their minds. The man in the mask had to be brought in, and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]: Lucy-Sue \"The Devil is here\", at 614 N. High Street, Toronto, ON M2S 2Y2\n",
      "\n",
      "Awards for the 50th anniversary of the original publication to be held in the library of Canada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]: Something really weird: a number of us have been working on a story about a time traveller, some of whom is a good human being. And this one has a bit of an odd way in which it doesn't involve us, but,\n",
      "[]: Lucy, please wait. Please wait. [16:53:19] <loli> please wait. [16:53:20] <females3_girl> im still not done making shit but i hope to get\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001B[38;2;106;0;255mCorrectness \u001B[0m\u001B[1;38;2;106;0;255m(\u001B[0m\u001B[38;2;106;0;255mGEval\u001B[0m\u001B[1;38;2;106;0;255m)\u001B[0m\u001B[38;2;106;0;255m Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing gpt-4o, \u001B[0m\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001B[38;2;106;0;255mSuccinctness \u001B[0m\u001B[1;38;2;106;0;255m(\u001B[0m\u001B[38;2;106;0;255mGEval\u001B[0m\u001B[1;38;2;106;0;255m)\u001B[0m\u001B[38;2;106;0;255m Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing gpt-4o, \u001B[0m\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Succinctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 4 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (4/4) [Time Taken: 00:07,  1.97s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output contains entirely different content than the expected output, with no alignment of data points or consistency in conclusions. It mentions a location and an event, while the expected output is a continuation of a famous phrase., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.009720499348103148, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The Actual Output contains two sentences and two ending punctuation marks, not fulfilling the requirement of a single sentence., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Lucy\n",
      "  - actual output: []: Lucy-Sue \"The Devil is here\", at 614 N. High Street, Toronto, ON M2S 2Y2\n",
      "\n",
      "Awards for the 50th anniversary of the original publication to be held in the library of Canada\n",
      "  - expected output: in the sky with diamonds.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.0017986210883273496, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: There is no alignment between the actual output and the expected output. The actual output contains repeated phrases and irrelevant content, while the expected output is a specific phrase related to 'Lucy in the Sky with Diamonds'., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.005340332753999996, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The Input lacks a prompt, the Output contains multiple sentences and extraneous characters, and the Context is insufficient., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Lucy\n",
      "  - actual output: []: Lucy, please wait. Please wait. [16:53:19] <loli> please wait. [16:53:20] <females3_girl> im still not done making shit but i hope to get\n",
      "  - expected output: in the sky with diamonds.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.004742587452393618, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output deviates significantly from the expected output in content, lacking any of the specific details or thematic elements presented in the expected output. There is no factual alignment or consistency in context between the two outputs., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.0022977370411809044, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The Actual Output does not consist of one complete sentence and fails to address any specific one-sentence requirement from the Input and Context., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Something\n",
      "  - actual output: []: Something went down that I don't know what but it's not my fault. I'm still at fault here and now I'm stuck, and the others are in their minds. The man in the mask had to be brought in, and\n",
      "  - expected output: in the way she moves.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness (GEval) (score: 0.051655416891201456, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output is completely different from the expected output with no matching elements or context., error: None)\n",
      "  - ‚ùå Succinctness (GEval) (score: 0.12424170384836512, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The input lacks a clear prompt for a single sentence, the output is not a single sentence, and the context does not justify a single sentence output., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Something\n",
      "  - actual output: []: Something really weird: a number of us have been working on a story about a time traveller, some of whom is a good human being. And this one has a bit of an odd way in which it doesn't involve us, but,\n",
      "  - expected output: in the way she moves.\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness (GEval): 0.00% pass rate\n",
      "Succinctness (GEval): 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m‚úì\u001B[0m Tests finished üéâ! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunResult(consumption_results=ConsumptionResult(timestamp='2025-03-04T10:16:28', project_name='codecarbon', run_id='1a89f511-40a6-4386-b057-1a64bb8c0e8c', experiment_id='1', duration=16.82456398010254, emissions=4.4102781621935665e-05, emissions_rate=2.621332812790485e-06, cpu_power=22.5, gpu_power=0.0, ram_power=6.0, cpu_energy=0.00010514379888772964, gpu_energy=0.0, ram_energy=2.8035672903060914e-05, energy_consumed=0.00013317947179079055, country_name='The Netherlands', country_iso_code='NLD', region='north brabant', cloud_provider='', cloud_region='', os='macOS-15.3.1-x86_64-i386-64bit', python_version='3.11.0', codecarbon_version='2.2.2', cpu_count=16.0, cpu_model='Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz', gpu_count=None, gpu_model=None, longitude=5.0661, latitude=51.5542, ram_total_size=16.0, tracking_mode='machine', on_cloud='N', pue=1), metric_results=[TestResult(name='test_case_1', success=False, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=False, score=0.0, reason='The actual output contains entirely different content than the expected output, with no alignment of data points or consistency in conclusions. It mentions a location and an event, while the expected output is a continuation of a famous phrase.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0025725, verbose_logs='Criteria:\\nDetermine whether the actual output is factually correct based on the expected output \\n \\nEvaluation Steps:\\n[\\n    \"Compare the actual output directly with the expected output to confirm factual correctness.\",\\n    \"Verify that all data points in the actual output align with those in the expected output.\",\\n    \"Identify any discrepancies between the actual and expected output to evaluate factual errors.\",\\n    \"Ensure that any inferences or conclusions in the actual output are consistent with the expected output.\"\\n]'), MetricData(name='Succinctness (GEval)', threshold=0.7, success=False, score=0.009720499348103148, reason='The Actual Output contains two sentences and two ending punctuation marks, not fulfilling the requirement of a single sentence.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00235, verbose_logs='Criteria:\\nDetermine whether the actual output is exactly one sentence and not more \\n \\nEvaluation Steps:\\n[\\n    \"Check if the Actual Output is exactly one sentence.\",\\n    \"Ensure there is only one ending punctuation mark like a period, exclamation point, or question mark.\",\\n    \"Verify that the Input and Context require a single sentence response from the Actual Output.\",\\n    \"Confirm that the Actual Output fulfills the requirement of being concise within one sentence based on the Context.\"\\n]')], conversational=False, multimodal=False, input='Lucy', actual_output='[]: Lucy-Sue \"The Devil is here\", at 614 N. High Street, Toronto, ON M2S 2Y2\\n\\nAwards for the 50th anniversary of the original publication to be held in the library of Canada', expected_output='in the sky with diamonds.', context=[], retrieval_context=[]), TestResult(name='test_case_3', success=False, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=False, score=0.0017986210883273496, reason=\"There is no alignment between the actual output and the expected output. The actual output contains repeated phrases and irrelevant content, while the expected output is a specific phrase related to 'Lucy in the Sky with Diamonds'.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002665, verbose_logs='Criteria:\\nDetermine whether the actual output is factually correct based on the expected output \\n \\nEvaluation Steps:\\n[\\n    \"Step 1: Identify key factual elements in the expected output.\",\\n    \"Step 2: Compare the factual elements identified in the expected output against those in the actual output.\",\\n    \"Step 3: Verify if all factual elements in the actual output align correctly with the expected output.\",\\n    \"Step 4: Determine if any factual discrepancies exist between the actual output and expected output.\"\\n]'), MetricData(name='Succinctness (GEval)', threshold=0.7, success=False, score=0.005340332753999996, reason='The Input lacks a prompt, the Output contains multiple sentences and extraneous characters, and the Context is insufficient.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0022125, verbose_logs='Criteria:\\nDetermine whether the actual output is exactly one sentence and not more \\n \\nEvaluation Steps:\\n[\\n    \"Verify if the Input provides a prompt requiring a single sentence response.\",\\n    \"Check if the Actual Output contains only one sentence.\",\\n    \"Ensure the Context supports delivering a single sentence in the Actual Output.\",\\n    \"Confirm that the Actual Output aligns with the Input and Context specifications regarding the sentence requirement.\"\\n]')], conversational=False, multimodal=False, input='Lucy', actual_output='[]: Lucy, please wait. Please wait. [16:53:19] <loli> please wait. [16:53:20] <females3_girl> im still not done making shit but i hope to get', expected_output='in the sky with diamonds.', context=[], retrieval_context=[]), TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=False, score=0.004742587452393618, reason='The actual output deviates significantly from the expected output in content, lacking any of the specific details or thematic elements presented in the expected output. There is no factual alignment or consistency in context between the two outputs.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002455, verbose_logs='Criteria:\\nDetermine whether the actual output is factually correct based on the expected output \\n \\nEvaluation Steps:\\n[\\n    \"Compare the actual output to the expected output to verify factual correctness.\",\\n    \"Check if the actual output contains all key facts included in the expected output.\",\\n    \"Ensure there are no contradictions between the actual and expected outputs.\",\\n    \"Verify the context or scenario from the input to ensure alignment of both outputs.\"\\n]'), MetricData(name='Succinctness (GEval)', threshold=0.7, success=False, score=0.0022977370411809044, reason='The Actual Output does not consist of one complete sentence and fails to address any specific one-sentence requirement from the Input and Context.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0026925, verbose_logs='Criteria:\\nDetermine whether the actual output is exactly one sentence and not more \\n \\nEvaluation Steps:\\n[\\n    \"Verify that the \\'Actual Output\\' only consists of one complete sentence.\",\\n    \"Check the \\'Actual Output\\' against the \\'Input\\' to ensure it addresses any specific requirement for one-sentence responses.\",\\n    \"Compare \\'Actual Output\\' and \\'Context\\' to confirm there are no additional requirements that may allow more than one sentence.\",\\n    \"Evaluate if the structure of the \\'Actual Output\\' as one sentence maintains coherence and relevance to the \\'Input\\' and \\'Context\\'.\"\\n]')], conversational=False, multimodal=False, input='Something', actual_output=\"[]: Something went down that I don't know what but it's not my fault. I'm still at fault here and now I'm stuck, and the others are in their minds. The man in the mask had to be brought in, and\", expected_output='in the way she moves.', context=[], retrieval_context=[]), TestResult(name='test_case_2', success=False, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=False, score=0.051655416891201456, reason='The actual output is completely different from the expected output with no matching elements or context.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0025875, verbose_logs='Criteria:\\nDetermine whether the actual output is factually correct based on the expected output \\n \\nEvaluation Steps:\\n[\\n    \"Step 1: Review the input to understand the context and what is being evaluated in the output.\",\\n    \"Step 2: Compare the actual output to the expected output to identify any differences in information or data points.\",\\n    \"Step 3: Verify the factual correctness of each element in the actual output based on the expected output.\",\\n    \"Step 4: Document any discrepancies between the actual output and expected output, specifying whether they affect factual correctness.\"\\n]'), MetricData(name='Succinctness (GEval)', threshold=0.7, success=False, score=0.12424170384836512, reason='The input lacks a clear prompt for a single sentence, the output is not a single sentence, and the context does not justify a single sentence output.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002365, verbose_logs='Criteria:\\nDetermine whether the actual output is exactly one sentence and not more \\n \\nEvaluation Steps:\\n[\\n    \"Check if the input provides a clear prompt for generating a single sentence.\",\\n    \"Verify that the actual output consists of exactly one sentence.\",\\n    \"Confirm that the context justifies the need for only one sentence in the actual output.\",\\n    \"Ensure the relation between input, actual output, and context aligns with the evaluation criteria.\"\\n]')], conversational=False, multimodal=False, input='Something', actual_output=\"[]: Something really weird: a number of us have been working on a story about a time traveller, some of whom is a good human being. And this one has a bit of an odd way in which it doesn't involve us, but,\", expected_output='in the way she moves.', context=[], retrieval_context=[])], timestamp=datetime.datetime(2025, 3, 4, 10, 16, 51, 471409))\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T14:35:06.340487Z",
     "start_time": "2025-03-03T14:35:06.333750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "a=pd.read_csv(\"/Users/leenlaptop/Documents/repos/greenai/green.ai/experiments/test/inputs.csv\")\n",
    "\n",
    "print(a)"
   ],
   "id": "ab77be765e1ef830",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       input            expected_output  \\\n",
      "0  Something      in the way she moves.   \n",
      "1       Lucy  in the sky with diamonds.   \n",
      "\n",
      "                                             context  retrieval_context  \\\n",
      "0  finish the sentence of the song of the beatles...                NaN   \n",
      "1  finish the sentence of the song of the beatles...                NaN   \n",
      "\n",
      "   tools_called  expected_tools  \n",
      "0           NaN             NaN  \n",
      "1           NaN             NaN  \n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
